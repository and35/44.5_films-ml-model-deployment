{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43debe34",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a042497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcd8513",
   "metadata": {},
   "source": [
    "## a) cargamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeb45838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicamos las rutas del dataset\n",
    "movies_data_path = '../dataset/original_data/movies.csv'\n",
    "finantial_data_path = '../dataset/original_data/finantials.csv'\n",
    "opening_data_path = '../dataset/original_data/opening_gross.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31c3b9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos los datasets en formato de dataframe \n",
    "fin_data = pd.read_csv(finantial_data_path)\n",
    "movie_data = pd.read_csv(movies_data_path)\n",
    "opening_data = pd.read_csv(opening_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef0c03",
   "metadata": {},
   "source": [
    "## b) creamos un solo dataset con todas las columnas y filas utiles \n",
    "en nuestro caso tenemos las columnas repartidas en 3 DF, solo couparaemos las columnas numericas asi que elimnamos las categoricas, el numero de registros varia asi que haremos merge para que solo se concerven los registros que existen en todas las tablas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0811c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraemos las columnas que vamos a utilizar (las numericas y la del titulo[sera el key del merge]) del DF movie\n",
    "\"\"\"numeric_columns_mask = (movie_data.dtypes == float) | (movie_data.dtypes == int)\n",
    "numeric_columns = [column for column in numeric_columns_mask.index if numeric_columns_mask[column]]\n",
    "movie_data = movie_data[numeric_columns+['movie_title']]\"\"\"\n",
    "movie_data = pd.concat([movie_data.select_dtypes(include=['float64', \"int64\"]) ,movie_data[\"movie_title\"]], \n",
    "       axis=1) # este es mi codigo que cree y es mas facil  axis=1 = unir columnas axis=0 = unir filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "697ea2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2304, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_data = fin_data[['movie_title','production_budget','worldwide_gross']] \n",
    "# hacemos un left para que se concerben los registros que existen solo en las 3 tablas\n",
    "fin_movie_data = pd.merge(fin_data, movie_data, on= 'movie_title', how='left') \n",
    "full_movie_data = pd.merge( opening_data,fin_movie_data, on = 'movie_title', how='left')\n",
    "full_movie_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c372e153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2304, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ignoraremos las peliculas que tienen 0 y null en dinero generado \n",
    "full_movie_data[(full_movie_data.worldwide_gross != 0) & (full_movie_data.worldwide_gross.notnull())].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbd03bcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# quitamos las columnas que no son utiles para el entrenamiento\n",
    "full_movie_data = full_movie_data.drop(['movie_title','gross'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68311214",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['opening_gross', 'screens', 'production_budget', 'worldwide_gross',\n",
       "       'title_year', 'aspect_ratio', 'duration', 'cast_total_facebook_likes',\n",
       "       'budget', 'imdb_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_movie_data.columns # vemos el DF final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eda321",
   "metadata": {},
   "source": [
    "en este punto ya tenemos un solo dataframe con toda la data que sera util para entrenar el modelo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d855446",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9699a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6827e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos nuestro input var y output var\n",
    "X = full_movie_data.drop(['worldwide_gross'], axis = 1)\n",
    "y = full_movie_data['worldwide_gross']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f943a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contruimos un pipeline simple \n",
    "pipeline = Pipeline([ # solo especifiamos el alias del modelo y el modelo \n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')), # completa los valores Null\n",
    "    ('core_model', GradientBoostingRegressor()) #modelo de entrenamiento\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cca7ef92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.7619803 , 0.77409816, 0.67874742, 0.65125394, 0.66451001,\n",
       "        0.79956365, 0.78478551, 0.73388386, 0.67225266, 0.79540873]),\n",
       " 'score_time': array([0.00400424, 0.00701118, 0.0040102 , 0.00401521, 0.00499701,\n",
       "        0.00585794, 0.00686502, 0.00699759, 0.00501227, 0.00597072]),\n",
       " 'test_score': array([0.67454511, 0.85166096, 0.64392674, 0.78093676, 0.78345327,\n",
       "        0.86422566, 0.76040778, 0.87655337, 0.68674236, 0.65732363]),\n",
       " 'train_score': array([0.91673951, 0.91581777, 0.9228721 , 0.91654412, 0.92172829,\n",
       "        0.91476722, 0.92151444, 0.91734995, 0.92320705, 0.91766026])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = cross_validate(pipeline ,X,y,return_train_score=True,cv=10) # cv = numero de bloques \n",
    "results # ahora tenemos los datos de puntuacion y tiempo para el fit y el test de las 10 iteraciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbde089f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.918820072167686\n",
      "Test Score: 0.7579775628432255\n"
     ]
    }
   ],
   "source": [
    "# obtenemos el promedio de las 10 iteraciones del crossvalidation \n",
    "train_score = np.mean(results['train_score'])\n",
    "test_score = np.mean(results['test_score'])\n",
    "assert train_score > 0.7 # asi se usa assert:\n",
    "assert test_score > 0.65 #  afirmo que train score es > 0.7 si no deten la ejecucion por que hay un error atras\n",
    "print(f'Train Score: {train_score}')\n",
    "print(f'Test Score: {test_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d780543",
   "metadata": {},
   "source": [
    "## Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5827e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da68e0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# de todos los parametros del modelo GBR definimos uno que sera ajustado y en que rango se haran pruebas\n",
    "param_tunning = {'core_model__n_estimators': range(20,501,20)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "193f0cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# volvemos a instanciar el pipeline de atras\n",
    "estimator = Pipeline([\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "    ('core_model', GradientBoostingRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2e5d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search= GridSearchCV(estimator, # el modelo o pipeline\n",
    "                        param_grid = param_tunning, # el paramatro a ajustar\n",
    "                        scoring='r2', # la metrica de validacion \n",
    "                        cv=5)         # numero de bloques de particion en el crossvalidation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba418e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacemos un hold out \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.35,random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95368333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('imputer', SimpleImputer()),\n",
       "                                       ('core_model',\n",
       "                                        GradientBoostingRegressor())]),\n",
       "             param_grid={'core_model__n_estimators': range(20, 501, 20)},\n",
       "             scoring='r2')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entrenamos el estimador\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f03a1dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacemos la tecnica de cross validation sobre el  mejor modelo (estimator) encontrado en grid_search \n",
    "final_result = cross_validate(grid_search.best_estimator_,X_train,y_train,return_train_score=True,cv=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "461f63dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.9667702920281013\n",
      "Test Score: 0.7628977122405335\n"
     ]
    }
   ],
   "source": [
    "# volvemos a hacer el promedio de las metricas del crossvalidation para el mejor modelo \n",
    "train_score = np.mean(final_result['train_score'])\n",
    "test_score = np.mean(final_result['test_score'])\n",
    "assert train_score > 0.7\n",
    "assert test_score > 0.65\n",
    "print(f'Train Score: {train_score}')\n",
    "print(f'Test Score: {test_score}')\n",
    "# la metrica de evaluacion es r^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c669f7d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('imputer', SimpleImputer()),\n",
       "  ('core_model', GradientBoostingRegressor(n_estimators=220))],\n",
       " 'verbose': False,\n",
       " 'imputer': SimpleImputer(),\n",
       " 'core_model': GradientBoostingRegressor(n_estimators=220),\n",
       " 'imputer__add_indicator': False,\n",
       " 'imputer__copy': True,\n",
       " 'imputer__fill_value': None,\n",
       " 'imputer__missing_values': nan,\n",
       " 'imputer__strategy': 'mean',\n",
       " 'imputer__verbose': 0,\n",
       " 'core_model__alpha': 0.9,\n",
       " 'core_model__ccp_alpha': 0.0,\n",
       " 'core_model__criterion': 'friedman_mse',\n",
       " 'core_model__init': None,\n",
       " 'core_model__learning_rate': 0.1,\n",
       " 'core_model__loss': 'squared_error',\n",
       " 'core_model__max_depth': 3,\n",
       " 'core_model__max_features': None,\n",
       " 'core_model__max_leaf_nodes': None,\n",
       " 'core_model__min_impurity_decrease': 0.0,\n",
       " 'core_model__min_samples_leaf': 1,\n",
       " 'core_model__min_samples_split': 2,\n",
       " 'core_model__min_weight_fraction_leaf': 0.0,\n",
       " 'core_model__n_estimators': 220,\n",
       " 'core_model__n_iter_no_change': None,\n",
       " 'core_model__random_state': None,\n",
       " 'core_model__subsample': 1.0,\n",
       " 'core_model__tol': 0.0001,\n",
       " 'core_model__validation_fraction': 0.1,\n",
       " 'core_model__verbose': 0,\n",
       " 'core_model__warm_start': False}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtenmso los parametros para el mejor modelo (estimator)\n",
    "grid_search.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7c8bdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### imputer:SimpleImputer()\n",
      "\n",
      "### core_model:GradientBoostingRegressor(n_estimators=220)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# este paso solo es para ver que sucedia en la funcion de save_simple_metrics_report en utils.py\n",
    "for key, value in grid_search.best_estimator_.named_steps.items():\n",
    "    print(f'### {key}:{value.__repr__()}'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d479277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos el pipeline final con el modelo y sus parametros mas optimos\n",
    "estimator = Pipeline([\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "    ('core_model', GradientBoostingRegressor(n_estimators=220,\n",
    "                                             alpha=0.9,\n",
    "                                             ccp_alpha=0.0,\n",
    "                                             criterion='friedman_mse',\n",
    "                                             init=None,\n",
    "                                             learning_rate=0.1,\n",
    "                                             loss='squared_error',\n",
    "                                             max_depth=3,\n",
    "                                             max_features=None,\n",
    "                                             max_leaf_nodes=None,\n",
    "                                             min_impurity_decrease=0.0,\n",
    "                                             min_samples_leaf=1,\n",
    "                                             min_samples_split=2,\n",
    "                                             min_weight_fraction_leaf=0.0,\n",
    "                                             n_iter_no_change=None,\n",
    "                                             random_state=None,\n",
    "                                             subsample=1.0,\n",
    "                                             tol=0.0001,\n",
    "                                             validation_fraction=0.1,\n",
    "                                             verbose=0,\n",
    "                                             warm_start=False))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04e3c693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('imputer', SimpleImputer()),\n",
       "                ('core_model', GradientBoostingRegressor(n_estimators=220))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entrenamos el modelo final\n",
    "estimator.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45e1d10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7290239326685686"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a507f828",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de4b0573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88ca4771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model/model.pkl']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# guardamos el modelo final ya entrenado en formato .pkl\n",
    "dump(estimator, '../model/model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1a1dbec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['opening_gross', 'screens', 'production_budget', 'title_year',\n",
       "       'aspect_ratio', 'duration', 'cast_total_facebook_likes', 'budget',\n",
       "       'imdb_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3729605",
   "metadata": {},
   "source": [
    "creamos un .txt con los diccionarios para hacer las pruebas usando FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1693200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "Xsample = X_test.sample(3, random_state=42)\n",
    "Xsample = Xsample.to_dict(orient=\"index\")\n",
    "\n",
    "Ysample = pd.DataFrame(y_test.sample(3, random_state=42))\n",
    "Ysample.columns = [\"worldwide_gross\"]\n",
    "Ysample = Ysample.to_dict(orient=\"index\")\n",
    "\n",
    "\n",
    "with open('../api/sample_to_test_API.txt', 'w') as convert_file:\n",
    "    convert_file.write(\"Tomamos 3 muestras aleatorias con la misma seed para hacer un test del modelo en FastAPI \\n \\n\")\n",
    "    convert_file.write(\"# input dict: \\n\")\n",
    "    convert_file.write(json.dumps(Xsample) + \"\\n \\n\")\n",
    "    convert_file.write(\"# output dict: \\n\")\n",
    "    convert_file.write(json.dumps(Ysample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ca6e4ccd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (1548479437.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [88], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    app_FA_path = os.environ.get('app_FAST','C:\\Users\\Panda\\Desktop\\platzi_code\\44.5_films-ml-model-deployment\\api\\main.py')\u001b[0m\n\u001b[1;37m                                                                                                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "app_FA_path = os.environ.get('app_FAST','C:\\Users\\Panda\\Desktop\\platzi_code\\44.5_films-ml-model-deployment\\api\\main.py')\n",
    "print(os.environ.get('app_FAST'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ecd54f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Panda\\\\Desktop\\\\platzi_code\\\\44.5_films-ml-model-deployment\\\\api\\\\main.py'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "os.environ['path_FASTAPI'] = r\"C:\\Users\\Panda\\Desktop\\platzi_code\\44.5_films-ml-model-deployment\\api\\main.py\"\n",
    "os.environ['path_FASTAPI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "74dce1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Panda\\Desktop\\platzi_code\\44.5_films-ml-model-deployment\\api\\main.py\n"
     ]
    }
   ],
   "source": [
    "path_FASTAPI = os.environ.get('path_FASTAPI', r\"C:\\Users\\Panda\\Desktop\\platzi_code\\44.5_films-ml-model-deployment\\api\\main.py\")\n",
    "print(path_FASTAPI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7dfc1af730c0cb5972b2f5ccb0fd55ca68302b818eb24f17c77ed7e3dfa5df81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
